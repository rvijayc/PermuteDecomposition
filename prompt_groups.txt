I would like to explore the problem of optimal decomposition of a torch.permute operation of an arbitrary dimension *contigous* torch tensor in terms of basic transpose operations. An arbitrary permute can be broken down into swaps of adjacent dimensions that can be implemented as transpose operations. For example, a permute of (3, 0, 1, 2) can be implemented in many ways of which a couple of possibilities are shown below:

1. Bubble swap of *adjacent* dimensions: (0, 1, 2, 3) -> (0, 1, 3, 2) -> (0, 3, 1, 2) -> (3, 0, 1, 2)
2. Swapping of *grouped and adjacent* dimensions: (0, 1, 2, 3) -> ((0, 1, 2), 3) -> (3, (0, 1, 2)) -> (3, 0, 1, 2)

In the second case, the operation "-> ((0, 1, 2), 3) -> (3, (0, 1, 2)) -> (3, 0, 1, 2) ->" involves a single transpose operation with added view functions to group dimensions. The second case is likely more efficient than the first one.

In addition, there are two types of transposes involved:

1. matrix_transpose => swaps fastest changing dimensions. For example: (d0, d1, d2) => (d0, d2, d1) where d2 is the fastest changing dimension in the original tensor.
2. block_transpose => swaps intermediate dimensions. For example (d0, d1, d2, d3) => (d0, d2, d1, d3) where d3 is the fastest changing dimension in the original tensor.

Lets assume that a matrix_transpose operation has a cost c1 times the number of bytes processed by the operation and a block_transpose operation has a cost of c2 times the number of bytes transferred.

I would like to design an optimal algorithm in terms of cost that finds the best sequence to implement a specific permutation using either single dimension or grouped dimension swaps with the following assumptions:

1. Only swaps of adjacent dimensions are allowed. "Adjacent" applies to both single or grouped dimension swaps.
    - For example, the grouped swap ((0, 1), (2, 3)) -> ((3, 2), (0, 1)) is a valid adjacent dimension swap.
2. The tensor should remain contiguous througout the process.

Any graph based algorithm (shortest path search, DP, etc.,) is fine. The algorithm should output the optimal swap sequence so that it can be executed at a later time. Add prints to highlight the paths you are exploring with corresponding costs, and add test cases to ensure that the answer you are getting by executing the optimal sequence matches the pytorch reference (torch.permute).

The function that "executes" a specific swap must be written in terms of the following operations:

- view operations => to implement "grouping" and "ungrouping" for swaps that use grouping.
- matrix_transpose(tensor) => that takes a 3D tensor of shape [d0, d1, d2] and swaps the 2 fastest-changing dimensions (d1, d2).
- block_transpose(tensor) => that takes a 4D tensor of shape [d0, d1, d2, d3] and swaps the middle two dimensions (d1, d2). Here, d3 is the fastest changing dimension.

I would prefer an object oriented approach to the solution.
